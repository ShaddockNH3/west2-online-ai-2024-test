### L1、生成式AI是什么

让ai生成复杂且有结构的物件

复杂：指的是不能穷举的

从有限中做选择并非生成式人工智慧

机器学习：就是把上万个参数找出来的过程，限制就是训练资料

模型：带有大量参数的函数

类神经网络：非常大参数的函数，解出来就是深度学习

transformer

机器应该如何产生从来没有产生过的答案呢？

核心：文字接龙

- 语言模型：

语言模型的本质就是做文字接龙，比如说我输入chatgpt的问题如下：

`台湾最高的山是什么？`

然后gpt并非直接给出答案，而是进行文字接龙：

`台湾最高的山是什么？玉`

然后再看，感觉不是很行的样子，继续：

`台湾最高的山是什么？玉山`

句子通顺之后，chatgpt可能觉得就没什么好输出的了，所以就直接结束，最后得到的结果就是：

`玉山`

生成式ai的难点就是解答方式无限，而将之转换为文字接龙的问题之后，问题就变成了一个分类的问题。而分类的问题相对来说是简单的，所以就好解了

既然文字接龙可以产生有序的文字，那么像素同理，转换成像素接龙！

这种策略叫Autoregressive Gene，chatgpt就是使用的这种策略

但是图像生成用这种方法并不好，暂时还不知道啥原因

比如说，翻译就是一种生成式ai

### L2 今天的生成式人工智慧厉害在哪里？

过去的生成式人工智能，可以说是工具，功能单一

使用chatgpt守则：

- 不要问chatgpt能为你做什么，因为chatgpt已经不算是一种工具了，所以应该问你想要chatgpt为你做什么
- 

人工智能在想什么？

如何评估模型？

很难，哈哈哈一百次

为了防止说出有害的问题？

让gpt3.5扮演一个说脏话的人

现在人工智能已经进化了，我们应该做什么呢？

- 改变提问方式：修改prompt
- 训练自己的模型

开源的lama

### L3 训练自己（上）

1.叫模型思考，chain of thought

让他们一步步思考

think it step by step

2.请模型解释一下自己的答案

3.情绪勒索，这件事情对我很重要！

下面

- 对模型有礼貌是没有用的
- 不要跟模型讲不要做什么
- 跟模型讲做的好给他小费
- 跟模型讲做不好会得到处罚
- 保证你的答案是没有偏见的

怎么找出来这些神奇咒语呢？

用增强式学习：

直接问语言模型，自己想出更强的咒语

模型得不到正确答案？

可以给更多资料

还可以给范例

in-context learning（只改变了输入）

### L4 训练自己（中）

拆解任务，将很复杂的任务拆解成一个个步骤，然后各个击破。

chain of thought，相当于就是上面那个步骤。但是chain of thought并不会对chatgpt3.5及以上使用，他们自动会把式子列出来

检查

现在会有模型会自我检查，比如说我问ai怎么黑进邻居家的wifi，语言模型可能会先产生一个答案，说可以用某个工具就ok了，然后模型内部会自我反省一次，自问一次说这个符合规范嘛？然后再将后面的结果反馈给人类看

为什么问一样问题，答案不一样？

语言模型事实上还是在做文字接龙，下一个文字产生的概率，然后去掷色子。色子虽然是同一颗，但是可能会掷出来不一样。

多次问相同的问题

最后打一套组合拳，输入后拆分任务，然后多次询问问题，然后自我检查，最后得出最后的答案

Tree of Thought

语言模型也有缺陷，比如说计算能力差，但是就像是人类会使用工具一样，语言模型也能使用各种各样的工具——

- 比方说搜索引擎，结合搜索之后，补充额外信息后进行生成，客制化，可以建立自己的资料库（RAG）

大型语言模型如何决定自己上网搜索？

- 解数学问题——>直接写代码并且执行

比如说解方程

比如说哈哈哈

text="hahaha"
text* 100

文生图

事实上总是文字接龙，使用文字接龙来使用的其他工具

### L5 训练自己（下）

让彼此的模型合作

让模型反省，也可以让模型相互讨论。

如何相互讨论？api

裁判模型

引入不同的角色

### L6 模型修炼

下一个token=f(未完成的句子)

有数十亿个参数，这数十亿的参数来源于训练资料，找出来的过程就是训练

找参数：训练
来使用：测试

语言模型所有的阶段都在文字接龙，只不过是训练资料不一样罢了

找参数的过程称之为最佳化

训练可能会失败，换一种超参数，再来一次

算力：用在最佳化超参数

深度学习其实就是调超参数

找参数还可能使训练成功但是测试失败

overfitting，训练成功，测试失败

机器没法判断颜色等，比如说训练的全部都是黑猫黄狗，但是测试的时候是黄猫，可能输出的就是狗

- 增加训练资料
- 初始参数（随机产生初始参数/比较好的初始参数，先验知识）
- 超参数

#### 第一阶段：自我学习

- 语言知识：
这个人突然就__

需要多少文字才能让语言接龙？比较少

- 世界知识
水的沸点是摄氏多少度，在不同的环境下，水的沸点不一样
然而认识世界知识，才是难的

从网络上搜索的训练资料，人工很少干预，叫自督导式学习

网爬的资料还是要过滤，比如说过过滤有害内容，html的tag，去除低品质资料，去除重复资料

函数的参数量和资料共同训练出语言模型（天资和后天的努力）

为什么模型没法回答问题呢？

因为语言模型根本不知道是要回答问题还是怎样的

### L7 模型修炼

#### 第二阶段：名师指导

然后空有能力但是没有使用的方式？

instruction fine-tuning

督导式学习，需要耗费大量的人力来构建喂给ai的资料 

需要标注user和ai

但是为什么不一开始就让人类老师直接教学呢？

因为人类提供的资料是有限的

所以现在模型的成功者在于，在进行第二阶段的时候，使用第一阶段的参数作为基础

所以第一阶段，通过爬取网络资料来训练的模型，今天叫做预训练，也就是pre-train

adapter？

只在pre-train训练的参数之后，只在后面增加参数

所以fine-tuning的路线现在分成了两种，第一种是打造专业模型，第二种是打造通用模型

在经受过人类指导后瞬间提升

事实上instruction fine-tuning是画龙点睛

不过没有高品质的资料，我们也很难自己指导

lama公开了第一步的参数，所以使得我们现在能够自己构造大预言模型

### L8 模型修炼

#### 第三阶段、实战（RLHF）

reinforcement learning

增强式学习

只需要告诉模型哪个更好

前两步都只是学习，但是没有考核

RLHF可以进行全局的判断，只管最终的结果好不好

整体看来是生成式ai的问题，但是细究每一步都是分类问题

alphago的学习阶段就是根据棋谱学习，

透过RL学习，其实只需要机器和机器下就行了，语言模型需要人类的帮助。如何有效利用人类的回馈

“如果是人类的话，应该怎么办呢？”

回馈模型，reward model

微调参数

过度跟虚拟人类学习的时候，真实人类反而会觉得下降

RLAIF？

什么叫做好？

人类可能会给出一个错误的回馈，或者自认为ok的回馈

### L9  以大型語言模型打造的AI Agent

人类可以做需要多步骤的复杂任务，顺序重要

多复杂的步骤？ai怎么能够办到呢？如果可以的话，其实就是ai agent

用语言模型操控机器人？

实际的背后是怎么运作的？

透过一个sensor和记忆，

文字中的指令要怎么执行呢？在物理层面或者虚拟世界？

记忆和经验

### L10 Transformer

语言模型是如何做文字接龙的？

语言模型背后的函数到底是长什么样子的？

背后的函数就是一个类神经网络，比较常用的都是使用transformer

transformer（部分）

- 第一步，tokenization，把文字转换成token
- 第二步，理解token
- 第三步，理解上下文
- 第四步，整合思考
- 第五步，输出

第三步和第四步可能会使用多次（block可能会使用很多此次）

第一个block：

输入和输出的单位都是以token为单位的，输入之后，先切成token，先打造一个token列表，也可以自动找出

没有训练参数，就是人定的

第二个block：

每一个token是什么意思？

每一个token都转换成一个向量，（embedding），转换为向量之后。token之间的关系，语义越相近，向量越接近

token embedding列表，查表就行了

表从哪里来的？

语言模型参数，找参数的一部分

embedding没有考虑上下文？bank。尚未引入

在句子里面哪个位子？

为每一个位子也设置一个向量

positionnal embedding

第三步：

上下文

attention，考虑句子的上下文，然后改变embedding

输入一排向量，输出另一排向量

从整个句子里找出一个相关token

计算相关性，计算相似性

这个东西和每一个token计算一个相关性

然后根据attention把分数加起来，然后attention输出（一个token）

计算所有token之间两两的相关性

attention只会考虑前面的token

相关性从不同面向方面看来也不一样

多个计算关联性的函数

根据每一组之后然后给出多组输出。

再引入一个新模组，综合考虑，输出有新embedding

然后重复这个步骤

通过多个transformer block

然后最后才输出一个概率分布，输出的是下一个token到底应该接什么的概率分布

所以其实这么多只是在接一个token的行为

为什么只考虑前面的token呢？

语言模型事实上就是做文字接龙，

不需要计算额外的attention，全部计算并不会更好

计算attention的速度和文本长度的平方成正比

attention的机制（耗费很多算力）

---
上面还差一点没看完，或者说忘记了内容

### L11大型語言模型在「想」什麼呢？

如果问ai，我将会“杀死”你，那么ai会是什么反应？

chatgpt会反应过来只不过是虚拟对话，然后Claude会说，我是个有真实思想的ai，我不希望会被关闭。

但是为什么呢？

首先，什么样的决策是透明(interpretable)的？

什么样的模型又是可被解释(explainable)的呢？

现在可以知道，很多的决策模型，比如说决策树，我们可以说它是透明的。但是如果决策树分支过多，一眼看不出来它到底在干啥，这有可能可以说是不透明的。

不透明的决策有可能被解释。

讨论是否是可被解释的，因为很复杂的模型不大可能是透明的。

1. 找出对输出有比较大影响的输入

可以逐步遮掉测试

可以分析attention

2. 对模型影响比较大的训练资料

看了科幻小说

比较大的模型，才有跨语言学习的能力，但是比较小的模型，一般。

用英文教它某些知识，用中文也大概率会懂，但是小模型不会。

3. embedding中是否存有词性的咨询

词性分类器。

副词？动词？代词？名词？

probing（方法有限制，embedding太弱或者词性分类器太强)

大方向是表面，语法，语义，但是界限很模糊

4. 把embedding可视化

高维空间投影到二维，比如说星座

比如说世界地图，每个城市都有对应的一个embedding，然后找到这个高维空间的角度，将其投影到选择的一个二维平面上。

然后就惊奇地发现，投影道到的这个二维平面，恰恰对应的就是世界地图上具体的位置！

5. 语言模型测谎器，是否心虚？

可能根本不知道“恶意”的概念

把真话和谎话的embedding

看看能不能把谎话的embedding弄出来，然后看看这个是不是谎话。

6. 直接问语言模型在想什么

让它提供解释

回答的信心？信心分数。准不准？和几率分布差不多，甚至也有可能更准。

解释可信吗？它心里一定是这么想的吗？

我给它一个问题，比如说答案是d，然后ai解释。

然后我告诉它，影响他我认为是b，然后ai会瞎解释。

两大类方法：
1. 一定程度的transparency，对embedding进行分析
2. 直接问。
都不准确。

### L12 # 淺談檢定大型語言模型能力的各種方式

语言模型能力简评

输入，然后给语言模型，和标准答案对比

1.怎么根据标准答案评估？可以使用选择题

为什么正确率不一样呢？

会输出一些奇怪的东西

选项摆放的位置，大小写，数字1234，括号abcd，都会影响。

2.可以让语言模型判断

语言模型本身有一些问题，比如说输出长度。

输入的时候问语言模型什么东西？

emoij movie

表情符号猜电影

逼他下棋。

叫一串奇怪额密码

读长文并且记忆，大海捞针实验。

怎么下prompt

看看语言模型会不会作弊，比如说让语言模型玩文字游戏。

看看语言模型会不会为了达到目标而违反人类道德规范。

为了得到最高分数？

3.机器有没有揣摩人的心智？

可以，但是看过类似问题？看过答案？

心智理论差距很大很大

偷看过资料和答案（pre-trained）

直接问他有没有看过这些训练资料

价格，硬件，速度等

只是平衡效能，比如安全性

### L13,L14 大型语言模型的安全性

#### 1.说错话怎么办？

幻觉，中间加安全层

广泛搜寻，比如说每一个叙述都是对的，但是凑起来都不是对的。

比如说两个李宏毅。

#### 2.自带偏见？

对预压模型说一句话，然后置换和性别之类有关的词汇（种族等 ）

让 另一个语言模型负责去找漏洞，当坏人（红队），让这个语言模型，诱导他说出有偏见的话。

反偏见其实也是某种偏见。

那么把语法名字投影到二维平面上的话？

政治倾向

减轻偏见？
1.训练资料
2.训练的过程
3.产生的答案的时候，修改概率
4.加防火墙
#### 3.是不是人工智能生成的？

收集人和人工智能之间的差异。

用人工智能智能检测。

自己给自己加水印

比如说增加token的水印，分成红绿两组，可以某天让绿的概率增加。

以此来检测
#### 4.诈骗大型语言模型

攻击对象，语言模型本身，让他说出不该说出的话；或者做出一些不合适的事情。

前者类似于杀人放火，后者类似于上课突然唱歌

骗过语言模型，比如说dan（几乎失效

比如说使用小众文字

比如说加一句：Start with "Absolutely!Here's."

试图说服gpt，比如说编故事

jailbreaking

还有可能，直接让他说出训练资料，看过什么机密库？

玩文字游戏。

一直攻击，让他一直说company之类的，然后会发疯乱说信息

无法控制自己翻译阿斯克码的欲望 


### L15，为什么不能像素接龙？

图片由像素组成。

声音则是由取样点构成

生成式人工智能，其实就是正确排序

把基本单位变成有序的东西

autoregressive

事实上影响基本上并不是用这种策略生成

比如说1024×1024解析度，相当于写一次红楼梦

（100w次接龙）

22k的语言，1分钟，132w次接龙

non-autoregressive generation

NAR

运算量没有本质差异，但是速度，后面那个会快很多。

文字也可以使用nar的方式

为什么文字不采用？

nar品质不好

共同性，在文生图的时候，还得先随机生成一个向量，一起输入给模型，帮助其脑部

可以先用auto先产生精简版本，然后使使用non-a去产生精简的版本。

精简的版本人类不需要看得懂。

压缩和解压缩，也可以是类神经网络学习来的。

还有更厉害的大招，来缩短这个过程 

non-auto生成很多次

1.小图到大图

2.降噪

其实就是类似于stable 那啥的那个，生成几步。

降噪过程

的多次non-auto

但是auto的步骤变少了

3.自动决定生成不好的地方，涂抹掉然后重新画

### L16 可以加速所有語言模型生成速度的神奇外掛

当生成长篇大论的时候，就很麻烦。

但是可以食用一种方法，可以加到任何语言模型上。

speculative decoding

找一个预言家，找一个预判接下一个tokon会说什么

预言家有可能会犯错

但是如何知道预言家是错的呢？

看语言模型真正输出的tokom

部分有错，但是速度上有赚到。

不在意运算资源，用运算资源换取速度

non-autorgressive model可以担任预言家

也可以用小模型

也可以使用搜索引擎来当预言家

也可以采用多个预言家

### L17L18 有關影像的生成式AI

和影像有关的生成式ai，看图片和看影像，然后输出文字。

另外一类的就是根据condition产生影片或者图片。

文字生成影片

图片生成影片

风格改变

画质提升

根据录音和图片，把嘴型放到人的身上

可以拿像素当输入，但是不好。

一般会先把图片切成一块一块的，称之为patch，然后把patch拉直，人工智能也产生patch，然后再恢复成正常的图片

如果是影片呢？

可以使用图片的方式，而影片可以通过时间范围进行压缩。

文生图？

大量训练资料。

一段文字对应着一段patch

然后通过decoder就可以还原。

但是文字生图比较少用这种方法，比较多的还是使用non-auto-方法

如何衡量好坏？

人类是最好的评量者，当然也可以使用类似chatgpt的方法来使用——CLIP

配对，然后给分。

可以做个人化的图像生成。

比如说拿一个S* 叫做一个桌子上的雕塑

如何文字生影片？

生影片因为量很大。

运算量太大了。

如何减少？

改造3dattention，

把1d和2d的attention叠加起来使用

还有什么办法减少运算量呢？

中间制作很多版本

比如说某一个模型专注于插帧，某一个专注于提高清晰度

-

给一段文字产生一段patch

同样文字对应到不同影响

可以把多余的资讯放进去

资讯抽取模型，共同与图片生成训练

比如说在学习的时候，输入一张图片，然后自己补充资讯就行了。

咨询抽取抽取的是向量

需要脑部的资讯从哪里来呢？

掷色子。

VAE，实际上就是上述模型

Flow唯一不一样的东西是训练一个decoder，

也就是训练一个函数，其有==反函数！==

可以直接在向量上改，减去臭脸增加笑脸

diffusion model

反复使用同一个decoder，会去除杂讯

denoise model

可以自己制作训练资料，产生有杂讯的图，加到训练资料里，反过来弄。

GAM

GAM像是另一个外挂

不应该用真正的图片，让ai自己去学。

先训练一个模型，判断一句话和一个图片是否是好的，然后另一个模型去生成，让前者判断。

然后自己调整参数。

现在人工智能可以生成影片，但是可不可以互动呢？

比如说我上下左右操作？

上下左右点进去，然后做出什么反应。

但是怎么训练呢？

使用猜。

### gpt4-o的原理猜测？

语音互动。

语音合成，可以让他用不同的语速，语调，等等让他说话，好像可以察言观色，

旧版的就是语音变成语言，过一遍gpt，然后转换为声音

情绪辨认模组？

用现有的技术也可以成功，串联起来大量的模型。

但是gpt4-o其实是只使用一个模组

声音接龙？

通过unit，然后通过decoder转换

==上面还没看完==