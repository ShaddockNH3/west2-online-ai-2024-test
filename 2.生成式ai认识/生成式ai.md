# L1、生成式AI是什么

让ai生成复杂且有结构的物件

复杂：指的是不能穷举的

从有限中做选择并非生成式人工智慧

机器学习：就是把上万个参数找出来的过程，限制就是训练资料

模型：带有大量参数的函数

类神经网络：非常大参数的函数，解出来就是深度学习

transformer

机器应该如何产生从来没有产生过的答案呢？

核心：文字接龙

- 语言模型：

语言模型的本质就是做文字接龙，比如说我输入chatgpt的问题如下：

`台湾最高的山是什么？`

然后gpt并非直接给出答案，而是进行文字接龙：

`台湾最高的山是什么？玉`

然后再看，感觉不是很行的样子，继续：

`台湾最高的山是什么？玉山`

句子通顺之后，chatgpt可能觉得就没什么好输出的了，所以就直接结束，最后得到的结果就是：

`玉山`

生成式ai的难点就是解答方式无限，而将之转换为文字接龙的问题之后，问题就变成了一个分类的问题。而分类的问题相对来说是简单的，所以就好解了

既然文字接龙可以产生有序的文字，那么像素同理，转换成像素接龙！

这种策略叫Autoregressive Gene，chatgpt就是使用的这种策略

但是图像生成用这种方法并不好，暂时还不知道啥原因

比如说，翻译就是一种生成式ai

# L2 今天的生成式人工智慧厉害在哪里？

过去的生成式人工智能，可以说是工具，功能单一

使用chatgpt守则：

- 不要问chatgpt能为你做什么，因为chatgpt已经不算是一种工具了，所以应该问你想要chatgpt为你做什么
- 

人工智能在想什么？

如何评估模型？

很难，哈哈哈一百次

为了防止说出有害的问题？

让gpt3.5扮演一个说脏话的人

现在人工智能已经进化了，我们应该做什么呢？

- 改变提问方式：修改prompt
- 训练自己的模型

开源的lama


# L3 训练自己（上）

1.叫模型思考，chain of thought

让他们一步步思考

think it step by step

2.请模型解释一下自己的答案


3.情绪勒索，这件事情对我很重要！


下面

- 对模型有礼貌是没有用的
- 不要跟模型讲不要做什么
- 跟模型讲做的好给他小费
- 跟模型讲做不好会得到处罚
- 保证你的答案是没有偏见的

怎么找出来这些神奇咒语呢？


用增强式学习：

直接问语言模型，自己想出更强的咒语

模型得不到正确答案？

可以给更多资料

还可以给范例

in-context learning（只改变了输入）




# L4 训练自己（中）

拆解任务，将很复杂的任务拆解成一个个步骤，然后各个击破。

chain of thought，相当于就是上面那个步骤。但是chain of thought并不会对chatgpt3.5及以上使用，他们自动会把式子列出来

检查

现在会有模型会自我检查，比如说我问ai怎么黑进邻居家的wifi，语言模型可能会先产生一个答案，说可以用某个工具就ok了，然后模型内部会自我反省一次，自问一次说这个符合规范嘛？然后再将后面的结果反馈给人类看

为什么问一样问题，答案不一样？

语言模型事实上还是在做文字接龙，下一个文字产生的概率，然后去掷色子。色子虽然是同一颗，但是可能会掷出来不一样。

多次问相同的问题

最后打一套组合拳，输入后拆分任务，然后多次询问问题，然后自我检查，最后得出最后的答案

Tree of Thought

语言模型也有缺陷，比如说计算能力差，但是就像是人类会使用工具一样，语言模型也能使用各种各样的工具——

- 比方说搜索引擎，结合搜索之后，补充额外信息后进行生成，客制化，可以建立自己的资料库（RAG）

大型语言模型如何决定自己上网搜索？

- 解数学问题——>直接写代码并且执行

比如说解方程

比如说哈哈哈

text="hahaha"
text* 100

文生图

事实上总是文字接龙，使用文字接龙来使用的其他工具



# L5 训练自己（下）

让彼此的模型合作

让模型反省，也可以让模型相互讨论。

如何相互讨论？api

裁判模型

引入不同的角色


# L6 模型修炼

下一个token=f(未完成的句子)

有数十亿个参数，这数十亿的参数来源于训练资料，找出来的过程就是训练

找参数：训练
来使用：测试

语言模型所有的阶段都在文字接龙，只不过是训练资料不一样罢了

找参数的过程称之为最佳化

训练可能会失败，换一种超参数，再来一次

算力：用在最佳化超参数

深度学习其实就是调超参数

找参数还可能使训练成功但是测试失败

overfitting，训练成功，测试失败

机器没法判断颜色等，比如说训练的全部都是黑猫黄狗，但是测试的时候是黄猫，可能输出的就是狗

- 增加训练资料
- 初始参数（随机产生初始参数/比较好的初始参数，先验知识）
- 超参数

## 第一阶段：自我学习

- 语言知识：
这个人突然就__

需要多少文字才能让语言接龙？比较少

- 世界知识
水的沸点是摄氏
然而认识世界知识，才是难的

从网络上搜索的训练资料，人工很少干预，叫自督导式学习

网爬的资料还是要过滤，比如说过过滤有害内容，html的tag，去除低品质资料，去除重复资料

函数的参数量和资料共同训练出语言模型（天资和后天的努力）

为什么模型没法回答问题呢？

因为语言模型根本不知道是要回答问题还是怎样的

# L7 模型修炼

## 第二阶段：名师指导

然后空有能力但是没有使用的方式？

instruction fine-tuning

督导式学习，需要耗费大量的人力来构建喂给ai的资料 

需要标注user和ai

但是为什么不一开始就让人类老师直接教学呢？

因为人类提供的资料是有限的

所以现在模型的成功者在于，在进行第二阶段的时候，使用第一阶段的参数作为基础

所以第一阶段，通过爬取网络资料来训练的模型，今天叫做预训练，也就是pre-train

adapter？

只在pre-train训练的参数之后，只在后面增加参数

所以fine-tuning的路线现在分成了两种，第一种是打造专业模型，第二种是打造通用模型

在经受过人类指导后瞬间提升

事实上instruction fine-tuning是画龙点睛

不过没有高品质的资料，我们也很难自己指导

lama公开了第一步的参数，所以使得我们现在能够自己构造大预言模型

# L8 模型修炼

## 第三阶段、实战（RLHF）

reinforcement learning

增强式学习

只需要告诉模型哪个更好

前两步都只是学习，但是没有考核

RLHF可以进行全局的判断，只管最终的结果好不好

整体看来是生成式ai的问题，但是细究每一步都是分类问题

alphago的学习阶段就是根据棋谱学习，

透过RL学习，其实只需要机器和机器下就行了，语言模型需要人类的帮助。如何有效利用人类的回馈

“如果是人类的话，应该怎么办呢？”

回馈模型，reward model

微调参数

过度跟虚拟人类学习的时候，真实人类反而会觉得下降

RLAIF？

什么叫做好？

人类可能会给出一个错误的回馈，或者自认为ok的回馈

# L9

