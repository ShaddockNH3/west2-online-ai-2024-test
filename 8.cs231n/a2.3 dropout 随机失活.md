难点也是公式以及对公示的理解应用。

**Dropout 前向传播与反向传播公式**

---

**前向传播公式**

训练阶段：
$$
\begin{aligned}
\text{随机生成掩码矩阵 } & M \in \{0,1\}^{n} \quad \text{（每个元素独立服从伯努利分布，保留概率为 } p\text{）} \\
\text{输出 } Y &= \frac{1}{p} M \odot X
\end{aligned}
$$

测试阶段：
$$
Y = X \quad \text{（不应用Dropout）}
$$

符号说明：
• $X \in \mathbb{R}^{n}$：输入向量（或矩阵）

• $M$：二进制掩码矩阵，$M_{i} \sim \text{Bernoulli}(p)$

• $\odot$：逐元素乘法（Hadamard积）

• $p$：神经元保留概率（通常设为0.5或0.7）


---

**反向传播公式**

训练阶段梯度计算：
$$
\frac{\partial L}{\partial X} = \frac{1}{p} M \odot \frac{\partial L}{\partial Y}
$$

测试阶段梯度计算：
$$
\frac{\partial L}{\partial X} = \frac{\partial L}{\partial Y} \quad \text{（与普通全连接层相同）}
$$

---

**核心作用**
| 阶段   | 行为                                                                 |
|--------|----------------------------------------------------------------------|
| 训练时 | 随机屏蔽神经元 → 强制网络学习鲁棒特征                                 |
| 测试时 | 使用全部神经元 → 输出稳定性更高                                      |

---

**示例说明**
假设输入向量 $X = [2.0, 4.0, 6.0]$，保留概率 $p = 0.5$，随机生成的掩码 $M = [1, 0, 1]$：

前向传播：
$$
Y = \frac{1}{0.5} \cdot [1,0,1] \odot [2.0,4.0,6.0] = [4.0, 0.0, 12.0]
$$

反向传播（假设上游梯度 $\frac{\partial L}{\partial Y} = [0.1, 0.2, 0.3]$）：
$$
\frac{\partial L}{\partial X} = \frac{1}{0.5} \cdot [1,0,1] \odot [0.1, 0.2, 0.3] = [0.2, 0.0, 0.6]
$$

---

**代码实现思路**
```python
class Dropout:
    def __init__(self, p=0.5):
        self.p = p
        self.mask = None

    def forward(self, X, training=True):
        if training:
            self.mask = (np.random.rand(*X.shape) < self.p) / self.p
            return X * self.mask
        else:
            return X

    def backward(self, dY):
        return dY * self.mask
```

---

**数学推导细节**
前向传播期望一致性
设输入特征值为 $x$，训练时输出期望为：
$$
\mathbb{E}[y_{\text{train}}] = \mathbb{E}\left[\frac{1}{p} m x\right] = \frac{1}{p} \cdot p \cdot x = x
$$

反向传播梯度修正
梯度计算需保持期望一致性：
$$
\mathbb{E}\left[\frac{\partial L}{\partial x}\right] = \mathbb{E}\left[\frac{1}{p} m \frac{\partial L}{\partial y}\right] = \frac{1}{p} \cdot p \cdot \frac{\partial L}{\partial y} = \frac{\partial L}{\partial y}

$$
