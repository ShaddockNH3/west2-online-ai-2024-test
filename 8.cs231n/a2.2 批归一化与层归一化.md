主要难点是公式

批归一化的前向传播与反向传播：

**Batch Normalization 前向传播公式**

**输入**：
• $\mathbf{X} \in \mathbb{R}^{N \times D}$：当前批量的输入（\( N \) 为批量大小，\( D \) 为特征维度）

• \( \gamma \in \mathbb{R}^{D} \)、\( \beta \in \mathbb{R}^{D} \)：可学习的缩放和偏移参数

• \( \epsilon \)：数值稳定常数（通常取 \( 1e\text{-}5 \)）


---

**前向传播步骤**：
1. 计算批量均值：
   $$
   \mu = \frac{1}{N} \sum_{i=1}^N X_i
   $$

2. 计算批量方差：
   $$
   \sigma^2 = \frac{1}{N} \sum_{i=1}^N (X_i - \mu)^2 + \epsilon
   $$

3. 标准化：
   $$
   \hat{X}_i = \frac{X_i - \mu}{\sqrt{\sigma^2}}
   $$

4. 缩放与偏移：
   $$
   Y_i = \gamma \odot \hat{X}_i + \beta
   $$

5. 更新移动平均（推理阶段使用）：
   $$
   \mu_{\text{running}} = \lambda \mu_{\text{running}} + (1-\lambda)\mu \\
   \sigma^2_{\text{running}} = \lambda \sigma^2_{\text{running}} + (1-\lambda)\sigma^2
   $$
   • \( \lambda \) 为动量系数（通常取 0.9）


---

**Batch Normalization 反向传播公式**

**输入**：
• \( dL/dY \in \mathbb{R}^{N \times D} \)：损失对输出的梯度

• 缓存：前向传播中保存的 \( \mu \)、\( \sigma^2 \)、\( \hat{X} \)、\( \gamma \)


---

**反向传播梯度计算**：
1. 对缩放参数 \( \gamma \) 的梯度：
   $$
   \frac{\partial L}{\partial \gamma} = \sum_{i=1}^N \frac{\partial L}{\partial Y_i} \odot \hat{X}_i
   $$

2. 对偏移参数 \( \beta \) 的梯度：
   $$
   \frac{\partial L}{\partial \beta} = \sum_{i=1}^N \frac{\partial L}{\partial Y_i}
   $$

3. 对输入 \( X \) 的梯度：
   $$
   \frac{\partial L}{\partial X_i} = \frac{\gamma}{\sqrt{\sigma^2}} \left( \frac{\partial L}{\partial Y_i} - \frac{1}{N} \left( \sum_{j=1}^N \frac{\partial L}{\partial Y_j} + \hat{X}_i \sum_{j=1}^N \frac{\partial L}{\partial Y_j} \odot \hat{X}_j \right) \right)
   $$

---

**代码实现思路**
**前向传播**：
```python
def batchnorm_forward(X, gamma, beta, eps=1e-5, momentum=0.9):
    mu = X.mean(axis=0)                 # 计算均值
    var = X.var(axis=0) + eps           # 计算方差
    X_hat = (X - mu) / np.sqrt(var)     # 标准化
    Y = gamma * X_hat + beta            # 缩放与偏移
    
    # 更新移动平均（仅在训练模式时更新）
    if training_mode:
        mu_running = momentum * mu_running + (1 - momentum) * mu
        var_running = momentum * var_running + (1 - momentum) * var
    return Y, (X, X_hat, mu, var, gamma, beta)
```

**反向传播**：
```python
def batchnorm_backward(dY, cache):
    X, X_hat, mu, var, gamma, beta = cache
    N, D = X.shape
    
    dgamma = np.sum(dY * X_hat, axis=0)   # 计算gamma的梯度
    dbeta = np.sum(dY, axis=0)            # 计算beta的梯度
    
    # 计算输入X的梯度
    dX_hat = dY * gamma
    dvar = np.sum(dX_hat * (X - mu) * (-0.5) * (var)**(-1.5), axis=0)
    dmu = np.sum(dX_hat * (-1 / np.sqrt(var)), axis=0) + dvar * (-2/N) * np.sum(X - mu, axis=0)
    dX = (dX_hat / np.sqrt(var)) + dvar * 2*(X - mu)/N + dmu/N
    return dX, dgamma, dbeta
```

---

**核心作用**
1. 加速训练：通过标准化输入分布，缓解梯度消失/爆炸问题。
2. 正则化效果：通过批量统计量引入噪声，减少对Dropout的依赖。
3. 允许更高的学习率：稳定梯度传播，提升模型收敛速度。

批归一化的前向传播和反向传播：

**层归一化（Layer Normalization）公式**

---

**前向传播公式**

输入：
• \( \mathbf{X} \in \mathbb{R}^{N \times D} \): 输入矩阵（\( N \) 为批量大小，\( D \) 为特征维度）

• \( \gamma \in \mathbb{R}^{D} \)、\( \beta \in \mathbb{R}^{D} \): 可学习的缩放和偏移参数

• \( \epsilon \): 数值稳定常数（通常取 \( 1e\text{-}5 \)）


---

前向传播步骤：
1. 计算层均值（沿特征维度）：
   $$
   \mu_i = \frac{1}{D} \sum_{j=1}^D X_{i,j} \quad \text{（对每个样本 \( i \in [1, N] \)）}
   $$

2. 计算层方差：
   $$
   \sigma_i^2 = \frac{1}{D} \sum_{j=1}^D (X_{i,j} - \mu_i)^2 + \epsilon
   $$

3. 标准化：
   $$
   \hat{X}_{i,j} = \frac{X_{i,j} - \mu_i}{\sqrt{\sigma_i^2}}
   $$

4. 缩放与偏移：
   $$
   Y_{i,j} = \gamma_j \cdot \hat{X}_{i,j} + \beta_j
   $$

---

**反向传播公式**

输入：
• \( \frac{\partial L}{\partial Y} \in \mathbb{R}^{N \times D} \): 损失对输出的梯度

• 缓存：前向传播中保存的 \( \mu \)、\( \sigma^2 \)、\( \hat{X} \)、\( \gamma \)


---

反向传播梯度计算：
1. 对缩放参数 \( \gamma \) 的梯度：
   $$
   \frac{\partial L}{\partial \gamma_j} = \sum_{i=1}^N \frac{\partial L}{\partial Y_{i,j}} \cdot \hat{X}_{i,j}
   $$

2. 对偏移参数 \( \beta \) 的梯度：
   $$
   \frac{\partial L}{\partial \beta_j} = \sum_{i=1}^N \frac{\partial L}{\partial Y_{i,j}}
   $$

3. 对输入 \( X \) 的梯度：
   $$
   \frac{\partial L}{\partial X_{i,j}} = \frac{\gamma_j}{\sqrt{\sigma_i^2}} \left( \frac{\partial L}{\partial Y_{i,j}} - \frac{1}{D} \sum_{k=1}^D \frac{\partial L}{\partial Y_{i,k}} \cdot \gamma_k \cdot \hat{X}_{i,k} \right) - \frac{\gamma_j}{D \sigma_i^2} \sum_{k=1}^D \frac{\partial L}{\partial Y_{i,k}} \cdot \hat{X}_{i,k}
   $$

---

**代码实现思路**
**前向传播**：
```python
def layernorm_forward(X, gamma, beta, eps=1e-5):
    mu = X.mean(axis=1, keepdims=True)          # 沿特征维度计算均值 (N,1)
    var = X.var(axis=1, keepdims=True) + eps     # 沿特征维度计算方差 (N,1)
    X_hat = (X - mu) / np.sqrt(var)              # 标准化 (N,D)
    Y = gamma * X_hat + beta                     # 缩放与偏移 (N,D)
    return Y, (X, X_hat, mu, var, gamma, beta)
```

**反向传播**：
```python
def layernorm_backward(dY, cache):
    X, X_hat, mu, var, gamma, beta = cache
    N, D = X.shape
    
    # 计算gamma和beta的梯度
    dgamma = np.sum(dY * X_hat, axis=0)          # (D,)
    dbeta = np.sum(dY, axis=0)                   # (D,)
    
    # 计算输入X的梯度
    dX_hat = dY * gamma                          # (N,D)
    dvar = np.sum(dX_hat * (X - mu) * (-0.5) * (var)**(-1.5), axis=1, keepdims=True)  # (N,1)
    dmu = -np.sum(dX_hat / np.sqrt(var), axis=1, keepdims=True) - 2 * dvar * np.mean(X - mu, axis=1, keepdims=True)
    dX = (dX_hat / np.sqrt(var)) + 2 * (X - mu) * dvar / D + dmu / D
    
    return dX, dgamma, dbeta
```

---

**核心作用**
1. 稳定特征分布：对每个样本的所有特征进行归一化，缓解内部协变量偏移问题。
2. 适用于序列模型：在RNN/Transformer中，处理变长序列时不受批量大小影响。
3. 与批量归一化的区别：
   • 归一化维度：层归一化沿特征维度归一化，批量归一化沿批量维度。

   • 推理行为：层归一化无需存储移动平均，直接计算当前样本统计量。

   • 适用场景：层归一化更适合序列模型（如Transformer），批量归一化更适合CNN。
