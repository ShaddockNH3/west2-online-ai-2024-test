### SSL（Self-supervised Learning）

bert

自己想办法在没有label的情况下，将资料分成两部分。一部分是模型的输入，一部分是模型的标注

把正确的输入的一部分随机变成以下两种

第一，替换成黑幕，第二，替换成随机的字

训练的结果就是可以看出这部分被替换的东西本来属于什么范围

next sentence prediction？

这种方法没什么用，可能是因为这个任务太简单了（

bert学到的是什么，盖住一些词汇做天空，预测两个句子是否能拼在一起。而后者没什么大用

输入一个sequence，前面放一个cls，

bert只会做填空题？

可以被用在其他任务上，这些 跟填空题不一定要有关





【【李宏毅】自监督学习 (Self-supervised Learning) BERT GPT】 https://www.bilibili.com/video/BV1fL4y1z7Pi/?share_source=copy_web&vd_source=3fbbb3c2ad24817002f9c39fad247a3b