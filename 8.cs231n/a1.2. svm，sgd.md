**1. 多类SVM损失函数（Hinge Loss）**
数学公式：
• 数据损失：

  $$L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + \Delta)$$
  其中：
  • $s_j = X_i W_j$（类别$j$的分数）

  • $\Delta = 1$（边际超参数）


• 正则化损失：

  $$R(W) = \lambda \sum_{k} W_k^2$$
  （L2正则化项，$\lambda$是正则化强度）

• 总损失：

  $$L = \frac{1}{N} \sum_{i=1}^N L_i + R(W)$$

梯度公式：
• 对错误类别$j \neq y_i$：

  $$\frac{\partial L_i}{\partial W_j} = \begin{cases}
    X_i & \text{if } s_j - s_{y_i} + \Delta > 0 \\
    0 & \text{otherwise}
  \end{cases}$$
  
• 对正确类别$y_i$：

  $$\frac{\partial L_i}{\partial W_{y_i}} = -\sum_{j \neq y_i} \frac{\partial L_i}{\partial W_j}$$

• 正则化梯度：

  $$\frac{\partial R}{\partial W} = 2\lambda W$$
  
**朴素实现 (`svm_loss_naive`)**  
```python
for i in range(num_train):
    scores = X[i].dot(W)  # 计算单个样本的分数
    correct_score = scores[y[i]]
    for j in range(num_classes):
        if j == y[i]: continue
        margin = scores[j] - correct_score + 1
        if margin > 0:
            loss += margin
            dW[:,j] += X[i]  # 错误类梯度
            dW[:,y[i]] -= X[i]  # 正确类梯度
    loss /= num_train
    loss += reg * np.sum(W * W)
    dW = dW / num_train + 2 * reg * W
```

**向量化实现 (`svm_loss_vectorized`)**  
```python
scores = X.dot(W)  # 矩阵乘法一次性计算所有分数
correct_scores = scores[np.arange(num_train), y].reshape(-1,1)
margins = np.maximum(0, scores - correct_scores + 1)  # 广播机制
margins[np.arange(num_train), y] = 0  # 清零正确类
binary = (margins > 0).astype(float)  # 梯度指示矩阵
dW = X.T.dot(binary) / num_train  # 梯度矩阵乘法
```

总的来说，这部分也是主要依托于公式以及对numpy机制的了解。

### sgd

SGD与传统梯度下降的主要区别在于每次更新参数时使用的是单个样本或一个小批量（mini-batch）的梯度，而不是整个数据集的梯度

其依托的基本公式如下：

$$
\theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta J(\theta_t; x_i, y_i)
$$
• $\theta_t$：第$t$次迭代的参数向量  

• $\eta$：学习率（learning rate）  

• $\nabla_\theta J(\theta_t; x_i, y_i)$：基于单个样本$(x_i, y_i)$的损失函数梯度  

代码正是随机随机采样小批量数据计算