### rnn简述

**定义与结构**  
RNN（Recurrent Neural Network）是一种专为处理**序列数据**设计的神经网络，其核心特点是具有循环连接，允许信息在时间步之间传递。其基本结构包含：
• **输入层**：接收当前时间步的输入（如文本中的单词、时间序列中的观测值）。
• **隐藏层**：维护**隐藏状态**（Hidden State），捕获历史信息并通过时间步传递。
• **输出层**：生成当前时间步的预测（可选，如语言模型中的下一个单词）。

**工作原理**  
1. **时间步展开**：RNN对序列的每个元素按顺序处理，每一步的隐藏状态 $h_t$ 依赖于当前输入$x_t$ 和前一步的隐藏状态$h_{t-1}$。
2. **状态更新公式**（以Vanilla RNN为例）：
   $$
   h_t = \tanh(W_x x_t + W_h h_{t-1} + b)
   $$
   • $tanh$ 激活函数：控制输出范围在$[-1, 1]$，增强非线性。
   • 权重共享：参数 $W_x$、$W_h$、$b$ 在所有时间步共享，减少参数量。

**应用场景**  
• **自然语言处理**：文本生成、机器翻译、情感分析。
• **时间序列预测**：股票价格预测、天气建模。
• **序列标注**：语音识别、命名实体识别。

**挑战与局限**  
1. **长距离依赖问题**：早期时间步的信息难以传递到后续时间步。
2. **梯度问题**：
   • **梯度消失**：反向传播时，梯度随时间步指数衰减，导致模型难以学习长期依赖。
   • **梯度爆炸**：梯度急剧增长，破坏参数稳定性。
3. **计算效率**：串行处理导致训练速度较慢。

**改进方案**  
• **LSTM（长短期记忆网络）**：引入门控机制（遗忘门、输入门、输出门），选择性保留和遗忘信息。
• **GRU（门控循环单元）**：简化版LSTM，合并部分门控结构，降低计算复杂度。
• **双向RNN**：结合正向和反向序列信息，增强上下文理解。

**总结**  
RNN通过循环结构捕捉序列数据的时序依赖性，是处理文本、语音等任务的基础模型。尽管存在梯度问题和长距离依赖限制，但其变体（如LSTM、GRU）通过结构优化显著提升了性能，广泛应用于序列建模领域。

【54 循环神经网络 RNN【动手学深度学习v2】】 https://www.bilibili.com/video/BV1D64y1z7CA/?share_source=copy_web&vd_source=3fbbb3c2ad24817002f9c39fad247a3b

【55 循环神经网络 RNN 的实现【动手学深度学习v2】】 https://www.bilibili.com/video/BV1kq4y1H7sw/?share_source=copy_web&vd_source=3fbbb3c2ad24817002f9c39fad247a3b

### 作业思路简述

rnn实际上就是多引入了一个时间参数。

最开始两个函数的实现实际上就是跟着公式敲即可，后两个函数的实现则是根据维度的操纵迭代来写即可。

本来我想的是将T维度转换至第一维，实际上不需要，直接操作即可。

在实现反向传播迭代函数的时候，我忽略了梯度需要叠加，导致了一系列错误。事实上RNN的权重 Wx​ 和 Wh​ 在所有时间步 ​**​共享参数​**​，因此梯度是所有时间步的累加和

后面的函数则是实现学习文本中的单词。第一个函数是将单词转换为向量


### 前向传播：  

$$
\mathbf{next\_h} = \tanh \big( \underbrace{\mathbf{x} \mathbf{W_x} + \mathbf{prev\_h} \mathbf{W_h} + \mathbf{b}}_{\text{未激活的线性输出 } \mathbf{affine}} \big)
$$


```python
def rnn_step_forward(x, prev_h, Wx, Wh, b):
    """Run the forward pass for a single timestep of a vanilla RNN using a tanh activation function.

    The input data has dimension D, the hidden state has dimension H,
    and the minibatch is of size N.

    Inputs:
    - x: Input data for this timestep, of shape (N, D)
    - prev_h: Hidden state from previous timestep, of shape (N, H)
    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)
    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)
    - b: Biases of shape (H,)

    Returns a tuple of:
    - next_h: Next hidden state, of shape (N, H)
    - cache: Tuple of values needed for the backward pass.
    """

    """使用tanh激活函数，运行普通RNN单个时间步的前向传播。

    输入数据的维度为D，隐藏状态的维度为H，
    小批量的大小为N。

    输入:
    - x: 当前时间步的输入数据，形状为 (N, D)
    - prev_h: 前一时间步的隐藏状态，形状为 (N, H)
    - Wx: 输入到隐藏层的权重矩阵，形状为 (D, H)
    - Wh: 隐藏层到隐藏层的权重矩阵，形状为 (H, H)
    - b: 偏置项，形状为 (H,)

    返回一个元组:
    - next_h: 下一个隐藏状态，形状为 (N, H)
    - cache: 包含反向传播所需值的元组
    """
    next_h, cache = None, None
    ##############################################################################
    # TODO: Implement a single forward step for the vanilla RNN. Store the next  #
    # hidden state and any values you need for the backward pass in the next_h   #
    # and cache variables respectively.                                          #
    ##############################################################################
    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

    # TODO: 实现普通RNN的单个前向步骤。将下一个隐藏状态和反向传播所需的          #
    # 值分别存储在next_h和cache变量中。                                          #
    
    affine=prev_h.dot(Wh)+x.dot(Wx)+b
    next_h=np.tanh(affine)
    cache=(x,prev_h,Wx,Wh,b,affine,next_h)

    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
    ##############################################################################
    #                               END OF YOUR CODE                             #
    ##############################################################################
    return next_h, cache
```

---

### 反向传播输入

假设从上游传递来的梯度为 $\frac{\partial L}{\partial \mathbf{next\_h}}$，需要计算以下梯度：  
1. $\frac{\partial L}{\partial \mathbf{W_x}}$  
2. $\frac{\partial L}{\partial \mathbf{W_h}}$  
3. $\frac{\partial L}{\partial \mathbf{b}}$  
4. $\frac{\partial L}{\partial \mathbf{x}}$  
5. $\frac{\partial L}{\partial \mathbf{prev\_h}}$  

---

### 反向传播步骤

1. **激活函数梯度**  
   先计算经过 $\tanh$ 的梯度：  
   $$
   \mathbf{daffine} = \frac{\partial L}{\partial \mathbf{next\_h}} \odot \big( 1 - \mathbf{next\_h}^2 \big)
   $$
   其中 $\odot$ 表示逐元素乘法，$1 - \mathbf{next\_h}^2$ 是 $\tanh$ 的导数。

2. **偏置项梯度 $\mathbf{b}$**  
   $$
   \frac{\partial L}{\partial \mathbf{b}} = \sum_{\text{样本轴}} \mathbf{daffine}
   $$
   沿批次维度（通常是第0维）求和。

3. **输入权重梯度 $\mathbf{W_x}$**  
   $$
   \frac{\partial L}{\partial \mathbf{W_x}} = \mathbf{x}^\top \mathbf{daffine}
   $$
   矩阵乘法维度：$(D \times N) \cdot (N \times H) \Rightarrow (D \times H)$.

4. **隐藏权重梯度 $\mathbf{W_h}$**  
   $$
   \frac{\partial L}{\partial \mathbf{W_h}} = \mathbf{prev\_h}^\top \mathbf{daffine}
   $$
   矩阵乘法维度：$(H \times N) \cdot (N \times H) \Rightarrow (H \times H)$.

5. **输入数据梯度 $\mathbf{x}$**  
   $$
   \frac{\partial L}{\partial \mathbf{x}} = \mathbf{daffine} \mathbf{W_x}^\top
   $$
   矩阵乘法维度：$(N \times H) \cdot (H \times D) \Rightarrow (N \times D)$.

6. **前一时间步隐藏状态梯度 $\mathbf{prev\_h}$**  
   $$
   \frac{\partial L}{\partial \mathbf{prev\_h}} = \mathbf{daffine} \mathbf{W_h}^\top
   $$
   矩阵乘法维度：$(N \times H) \cdot (H \times H) \Rightarrow (N \times H)$.


```python
def rnn_step_backward(dnext_h, cache):
    """Backward pass for a single timestep of a vanilla RNN.

    Inputs:
    - dnext_h: Gradient of loss with respect to next hidden state, of shape (N, H)
    - cache: Cache object from the forward pass

    Returns a tuple of:
    - dx: Gradients of input data, of shape (N, D)
    - dprev_h: Gradients of previous hidden state, of shape (N, H)
    - dWx: Gradients of input-to-hidden weights, of shape (D, H)
    - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)
    - db: Gradients of bias vector, of shape (H,)
    """
    dx, dprev_h, dWx, dWh, db = None, None, None, None, None
    ##############################################################################
    # TODO: Implement the backward pass for a single step of a vanilla RNN.      #
    #                                                                            #
    # HINT: For the tanh function, you can compute the local derivative in terms #
    # of the output value from tanh.                                             #
    ##############################################################################
    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

    x,prev_h,Wx,Wh,b,affine,next_h=cache

    daffine=dnext_h*(1-next_h**2) #逐个元素相乘
    db=np.sum(daffine,axis=0) #这里注意一下，操作原本有点问题
    dWx=x.T.dot(daffine)
    dWh=prev_h.T.dot(daffine)
    dx=daffine.dot(Wx.T)
    dprev_h=daffine.dot(Wh.T)

    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
    ##############################################################################
    #                               END OF YOUR CODE                             #
    ##############################################################################
    return dx, dprev_h, dWx, dWh, db
```

---

### 前向传播迭代

```python
def rnn_forward(x, h0, Wx, Wh, b):
    """Run a vanilla RNN forward on an entire sequence of data.
    
    We assume an input sequence composed of T vectors, each of dimension D. The RNN uses a hidden
    size of H, and we work over a minibatch containing N sequences. After running the RNN forward,
    we return the hidden states for all timesteps.

    Inputs:
    - x: Input data for the entire timeseries, of shape (N, T, D)
    - h0: Initial hidden state, of shape (N, H)
    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)
    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)
    - b: Biases of shape (H,)

    Returns a tuple of:
    - h: Hidden states for the entire timeseries, of shape (N, T, H)
    - cache: Values needed in the backward pass
    """

    """在整个数据序列上运行普通RNN的前向传播。
    
    我们假设输入序列由T个维度为D的向量组成。RNN使用隐藏层大小为H，
    处理包含N个序列的小批量数据。运行RNN前向传播后，
    返回所有时间步的隐藏状态。

    输入:
    - x: 整个时间序列的输入数据，形状为 (N, T, D)
    - h0: 初始隐藏状态，形状为 (N, H)
    - Wx: 输入到隐藏层的权重矩阵，形状为 (D, H)
    - Wh: 隐藏层到隐藏层的权重矩阵，形状为 (H, H)
    - b: 偏置项，形状为 (H,)

    返回元组包含:
    - h: 整个时间序列的隐藏状态，形状为 (N, T, H)
    - cache: 反向传播所需的中间变量
    """
    h, cache = None, None
    ##############################################################################
    # TODO: Implement forward pass for a vanilla RNN running on a sequence of    #
    # input data. You should use the rnn_step_forward function that you defined  #
    # above. You can use a for loop to help compute the forward pass.            #
    ##############################################################################
    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

    # TODO: 为处理序列数据的普通RNN实现前向传播。                                #
    # 应使用你之前定义的rnn_step_forward函数。                                   #
    # 可以使用循环来帮助计算前向传播。                                          #
    
    # 其实本来的想法是1维和2维翻转，但是很麻烦。所以就改成了这样。

    N, T, D = x.shape
    H=h0.shape[1]
    h=np.zeros((N, T, H))
    h_prev=h0
    caches=[]

    for t in range(T):
      x_t=x[:,t,:]
      h_next,cache_t=rnn_step_forward(x_t, h_prev, Wx, Wh, b)
      h[:,t,:]=h_next
      caches.append(cache_t)
      h_prev=h_next

    cache=(x,h0,Wx,Wh,b,caches)

    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
    ##############################################################################
    #                               END OF YOUR CODE                             #
    ##############################################################################
    return h, cache
```


### 反向传播迭代

```python
def rnn_backward(dh, cache):
    """Compute the backward pass for a vanilla RNN over an entire sequence of data.

    Inputs:
    - dh: Upstream gradients of all hidden states, of shape (N, T, H)
    
    NOTE: 'dh' contains the upstream gradients produced by the 
    individual loss functions at each timestep, *not* the gradients
    being passed between timesteps (which you'll have to compute yourself
    by calling rnn_step_backward in a loop).

    Returns a tuple of:
    - dx: Gradient of inputs, of shape (N, T, D)
    - dh0: Gradient of initial hidden state, of shape (N, H)
    - dWx: Gradient of input-to-hidden weights, of shape (D, H)
    - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)
    - db: Gradient of biases, of shape (H,)
    """
    dx, dh0, dWx, dWh, db = None, None, None, None, None
    ##############################################################################
    # TODO: Implement the backward pass for a vanilla RNN running an entire      #
    # sequence of data. You should use the rnn_step_backward function that you   #
    # defined above. You can use a for loop to help compute the backward pass.   #
    ##############################################################################
    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

    x,h0,Wx,Wh,b,caches=cache # cache
    N,T,H=dh.shape
    D=x.shape[2]

    dx=np.zeros((N,T,D))
    dh0=np.zeros_like(h0)
    dWx=np.zeros_like(Wx)
    dWh=np.zeros_like(Wh)
    db=np.zeros_like(b)
    dh_next= np.zeros_like(h0)

    for t in reversed(range(T)):
      # 当前时间步的梯度=上游梯度 + 来自下一时间步的梯度
      dh_current = dh[:,t,:] + dh_next
        
      # 执行单步反向传播
      dx_t, dh_prev, dWx_t, dWh_t, db_t = rnn_step_backward(dh_current, caches[t])
        
      # 存储输入梯度
      dx[:,t,:] = dx_t
      
      # 累加参数梯度
      dWx += dWx_t
      dWh += dWh_t
      db += db_t
        
      # 传递梯度到前一时间步
      dh_next = dh_prev

    dh0 = dh_next
      
  
    '''
    dx[:,t,:],dh,dWx,dWh,db=rnn_step_backward(dh_prev, cache[t])
    dh_prev=dh
    '''

    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
    ##############################################################################
    #                               END OF YOUR CODE                             #
    ##############################################################################
    return dx, dh0, dWx, dWh, db
```

### 词嵌入：前向传播

#### **NumPy索引机制**

##### **基础索引**  
• 若 `W` 是二维数组 `(V, D)`，`x` 是标量索引 `i`，则 `W[i]` 返回第 `i` 行的向量（形状 `(D,)`）。
##### **扩展至多维索引**  
• 若 `x` 是二维数组 `(N, T)`，`W[x]` 的每个元素 `x[i,j]` 会被解释为行索引，结果是一个三维数组 `(N, T, D)`，其中：
  • `out[i, j, :] = W[x[i, j]]`

**底层实现**：  
• NumPy将 `x` 展平为一维数组，执行索引后，再恢复原始形状 `(N, T)`，并附加最后一维 `D`。

本问正是利用了该机制，**通过索引从权重矩阵 `W` 中提取对应的词向量**：
• **输入索引矩阵 `x`**：形状为 `(N, T)`，包含每个位置的单词索引（整数）。
• **权重矩阵 `W`**：形状为 `(V, D)`，其中第 `i` 行对应索引为 `i` 的单词的嵌入向量。
• **索引操作 `W[x]`**：使用 `x` 中的每个元素作为行索引，从 `W` 中提取对应的行向量。

**示例**：  
假设 `x` 是一个 `2x3` 的矩阵，内容为：
```python
x = [[5, 2, 7],
     [0, 3, 1]]
```
`W` 是一个 `10x4` 的矩阵（V=10，D=4），则 `W[x]` 的结果是一个 `2x3x4` 的三维数组，其中：
• `out[0,0,:] = W[5]` （第5行的4维向量）
• `out[0,1,:] = W[2]` （第2行的4维向量）
• `out[1,2,:] = W[1]` （第1行的4维向量）

其实简单来说，就是以W的第一个维度V为索引，生成一个新的矩阵

```python
def word_embedding_forward(x, W):
    """Forward pass for word embeddings.
    
    We operate on minibatches of size N where
    each sequence has length T. We assume a vocabulary of V words, assigning each
    word to a vector of dimension D.

    Inputs:
    - x: Integer array of shape (N, T) giving indices of words. Each element idx
      of x muxt be in the range 0 <= idx < V.
    - W: Weight matrix of shape (V, D) giving word vectors for all words.

    Returns a tuple of:
    - out: Array of shape (N, T, D) giving word vectors for all input words.
    - cache: Values needed for the backward pass
    """
    out, cache = None, None
    ##############################################################################
    # TODO: Implement the forward pass for word embeddings.                      #
    #                                                                            #
    # HINT: This can be done in one line using NumPy's array indexing.           #
    ##############################################################################
    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

    """词嵌入前向传播。
    
    处理包含N个样本的小批量数据，每个样本序列长度为T。
    假设词汇表大小为V，每个词被映射到维度为D的向量。

    输入:
    - x: 形状为 (N, T) 的整数数组，表示单词的索引。每个元素 idx
      必须满足 0 <= idx < V。
    - W: 形状为 (V, D) 的权重矩阵，包含所有单词的词向量。

    返回元组:
    - out: 形状为 (N, T, D) 的数组，包含输入单词对应的词向量。
    - cache: 反向传播所需的中间变量
    """

    # 提示：可以使用NumPy的数组索引功能在一行代码中完成。

    out=W[x] # N*T*D
    cache=(x,W)

    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
    ##############################################################################
    #                               END OF YOUR CODE                             #
    ##############################################################################
    return out, cache
```

现在out是一个$N*T*D$的矩阵 

---

### 词嵌入：反向传播

反向传播即要求通过dout和cache求dW，公式如下

$$
\frac{\partial \mathcal{L}}{\partial W[k, :]} = \sum_{(i,j) \in \{(i,j) \mid x_{i,j} = k\}} \frac{\partial \mathcal{L}}{\partial \text{out}[i, j, :]}
$$

• $W \in \mathbb{R}^{V \times D}$：词嵌入矩阵
• $x \in \mathbb{Z}^{N \times T}$：输入索引矩阵，$x_{i,j} \in \{0,1,...,V-1\}$
• $\frac{\partial \mathcal{L}}{\partial \text{out}} \in \mathbb{R}^{N \times T \times D}$：上游梯度
• 对词汇表中第$k$个词，梯度是所有输入位置$(i,j)$满足$x_{i,j}=k$的梯度之和

而`np.add.at`数学等价为：
$$
dW[k, :] \leftarrow dW[k, :] + \sum_{m \in \{m \mid x[m] = k\}} \text{dout}[m, :] \quad \forall k \in \{0, 1, \dots, V-1\}
$$


• $\text{dout} \in \mathbb{R}^{(N \cdot T) \times D}$：展平后的上游梯度
• $x \in \mathbb{Z}^{N \cdot T}$：展平后的索引矩阵
• 该操作将$\text{dout}$中所有与索引$k$关联的梯度累加到$dW[k,:]$

```python
def word_embedding_backward(dout, cache):
    """Backward pass for word embeddings.
    
    We cannot back-propagate into the words
    since they are integers, so we only return gradient for the word embedding
    matrix.

    HINT: Look up the function np.add.at

    Inputs:
    - dout: Upstream gradients of shape (N, T, D)
    - cache: Values from the forward pass

    Returns:
    - dW: Gradient of word embedding matrix, of shape (V, D)
    """
    dW = None
    ##############################################################################
    # TODO: Implement the backward pass for word embeddings.                     #
    #                                                                            #
    # Note that words can appear more than once in a sequence.                   #
    # HINT: Look up the function np.add.at                                       #
    ##############################################################################
    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

    """
    词嵌入的反向传播。
    
    由于单词是整数索引，无法对它们进行反向传播，
    因此仅返回词嵌入矩阵的梯度。

    提示：查阅函数 np.add.at

    输入:
    - dout: 上游梯度，形状为 (N, T, D)
    - cache: 前向传播保存的中间变量

    返回:
    - dW: 词嵌入矩阵的梯度，形状为 (V, D)
    """

    # TODO: 实现词嵌入的反向传播。                                               #
    #                                                                            #
    # 注意：同一个单词可能在序列中出现多次。                                    #
    # 提示：查阅函数 np.add.at                                                   #

    x,W=cache
    V,D=W.shape
    dW=np.zeros_like(W)

    indices=x.flatten()        #(N*T,)
    gradients=dout.reshape(-1,D)   #(N*T,D)

    np.add.at(dW,indices,gradients)

    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
    ##############################################################################
    #                               END OF YOUR CODE                             #
    ##############################################################################
    return dW
```

思路是展平后一次相加。

